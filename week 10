// Import necessary libraries
import org.apache.spark.mllib.clustering.KMeans
import org.apache.spark.mllib.linalg.Vectors
import org.apache.spark.mllib.regression.LabeledPoint
import org.apache.spark.sql.SQLContext

// Initialize SQLContext (available in Spark 1.6.0)
val sqlContext = new SQLContext(sc)

// Sample dataset: An RDD of LabeledPoint (label, features)
val data = sc.parallelize(Seq(
  LabeledPoint(0.0, Vectors.dense(1.0, 1.0)),
  LabeledPoint(0.0, Vectors.dense(1.1, 1.1)),
  LabeledPoint(1.0, Vectors.dense(3.0, 3.0)),
  LabeledPoint(1.0, Vectors.dense(3.1, 3.1)),
  LabeledPoint(2.0, Vectors.dense(5.0, 5.0)),
  LabeledPoint(2.0, Vectors.dense(5.1, 5.1))
))

// Convert to DataFrame (so we can use SQLContext's read format if needed in future)
val dataFrame = sqlContext.createDataFrame(data)

// Convert to RDD of Vectors (KMeans expects an RDD of Vectors)
val parsedData = dataFrame.rdd.map(row => row.getAs[org.apache.spark.ml.linalg.Vector]("features"))

// Initialize KMeans clustering
val kmeans = new KMeans()
  .setK(3)                // Number of clusters
  .setMaxIterations(10)   // Set the maximum number of iterations
  .setSeed(1L)            // Random seed for initialization

// Train the model
val model = kmeans.run(parsedData)

// Make predictions (assign each point to a cluster)
val predictions = parsedData.map(point => (point, model.predict(point)))

// Print the predictions
println("Predictions:")
predictions.take(10).foreach(println)  // Show first 10 predictions

// Print Cluster Centers
println("Cluster Centers: ")
model.clusterCenters.foreach(println)
